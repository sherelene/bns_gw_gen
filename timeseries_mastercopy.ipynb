{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timeseries_mastercopy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DP7HwAAI4En"
      },
      "source": [
        "#This code is meant to generate thousands of analytical waveforms structured with numerical simulations as a training set for a research project using a generative adversarial network. It integrates gravitational-wave specific and Bayesian inference python packages to produce data within a time-domain. The saved and appended data from the automated waveform simulations and associated metadata is done through the use of pandas and csv. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4RLUDeDf2v"
      },
      "source": [
        "Originally used \"!pip install pycbc lalsuite ligo-common\" and \"!pip install \"gwpy\" but something about installing both pycbc and gwpy causes colab to give errors in compatibilities, as well as, \"missing\" packages that asks you to restart runtime #so i separated gwpy and pcybc into different cells\n",
        "installing an older version of gwpy or else a compatibility error happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x0tyVUhCVFi"
      },
      "source": [
        "! pip install -q 'lalsuite==6.66' 'gwpy==1.0.1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ByAvQCfD8Yw"
      },
      "source": [
        "running cell that imports gwpy near the top so if there's any errors, there won't be time wasted restarting the runtime and running cells again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0h8IFwLfBpI"
      },
      "source": [
        "import gwpy\n",
        "from gwpy.timeseries import TimeSeries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRXMoY7HBLRk"
      },
      "source": [
        "#clone github repo of bajes\n",
        "\n",
        "! git clone https://github.com/matteobreschi/bajes.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXePtjaxBmW3"
      },
      "source": [
        "#installs bajes repo to your content files on google colab, if using linux itll \n",
        "#have its own directory\n",
        "\n",
        "%cd bajes\n",
        "! ls\n",
        "! python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFRkA4cCo6h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e9df8c-cef2-49be-958b-671e88c3c6ec"
      },
      "source": [
        "#necessary to run approx that uses \"TEOBResumS\"\n",
        "! git clone https://RoxGamba@bitbucket.org/eob_ihes/teobresums.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'teobresums'...\n",
            "Receiving objects: 100% (12864/12864), 72.08 MiB | 27.84 MiB/s, done.\n",
            "Resolving deltas: 100% (9410/9410), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsC6g8Wko9aF"
      },
      "source": [
        "#TEOBResumS wont install properly unless installing the build-essentials and lib*\n",
        "\n",
        "! apt update\n",
        "! apt install build-essential\n",
        "! apt-get install -y libconfig-dev\n",
        "!apt-get install libgsl-dev\n",
        "% cd teobresums/Python/\n",
        "!python TEOBResumSWrap_setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIQ79R4HCljv"
      },
      "source": [
        "#Importing some packages\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#depending on how bajes uploads on google colab, the path for bajes might be \n",
        "#different. So instead of \"bajes.bajes.obs...\" it might just be \"bajes.obs...\"\n",
        "\n",
        "from bajes.bajes.obs.gw import Series\n",
        "import csv\n",
        "\n",
        "#import google drive so files we are making wont disappear after runtime is over \n",
        "#this is unnecessary if not using google colab\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-GlhRRGEcw0"
      },
      "source": [
        "As of this moment there are 11 known events detected from the advanced gravitational-wave detectors we can access the data from the known events from the gwpy package and the time of event currently listed is from GW150914\n",
        "Waveform approximants(bottom of this cell) can be run independently from the event data, however, hard coding/replacing \"'time'\", \"hpc.plus\", etc and \n",
        "other adjustments of the code made it difficult for me to change the central time to equal zero so accessing the data was easier\n",
        "\n",
        "should the need to not use the data from events occur, comment out everything above \"#set the data properties\", comment out \"series\", and add: \n",
        "\n",
        "dT     = 1/srate\n",
        "df = 1/seglen\n",
        "f = np.arange(0, srate/2, df)\n",
        "wave  = Waveform(f, srate, seglen, 'NRPM')\n",
        "\n",
        "then in the next cell, under\n",
        "\n",
        "hp, hc = wave.compute_hphc(params)\n",
        "\n",
        "add\n",
        "\n",
        "t = np.array(range(0, len(hp)))*dT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJOgMXMV1E"
      },
      "source": [
        "time_of_event = 1126259462.4\n",
        "\n",
        "post_trigger_duration =4\n",
        "duration = 8\n",
        "analysis_start = time_of_event + post_trigger_duration - duration\n",
        "\n",
        "# Use gwpy to fetch the open data\n",
        "H1_analysis_data = TimeSeries.fetch_open_data(\n",
        "    \"H1\", analysis_start, analysis_start + duration, sample_rate=4096, \n",
        "    cache=True)\n",
        "\n",
        "t = H1_analysis_data.times\n",
        "strain = H1_analysis_data.value\n",
        "\n",
        "# set the data properties \n",
        "seglen = 8           # duration of the segment [s]\n",
        "srate  = 4096  * 16       # sampling rate [Hz]\n",
        "t_gps  = 0  # central value of time\n",
        "f_max  = 1024 * 4\n",
        "f_min  = 20 \n",
        "\n",
        "series = Series('time', strain, seglen=seglen, srate=srate, t_gps=t_gps, \n",
        "                f_min=f_min, f_max=f_max)\n",
        "print('here')\n",
        "\n",
        "from bajes.bajes.obs.gw import Waveform\n",
        "\n",
        "wave  = Waveform(series.freqs, srate, seglen, 'TEOBResumS_NRPM') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD1xO7KJVvC_"
      },
      "source": [
        "#if you get an error while using this cell with google colab, just restart the\n",
        "#cell and not the whole kernal\n",
        "!pip install pycbc\n",
        "import pylab\n",
        "\n",
        "from pycbc import waveform\n",
        "from pycbc.types import TimeSeries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNTl2OSBSBwK"
      },
      "source": [
        "#since we want a lot of waveforms, we run our this cell with a for loop\n",
        "#change the range number based on how many sims you want\n",
        "\n",
        "for num in range(1):\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "\n",
        "#This part of code is from Dr. Phillip Landry and what it does is deternmine a\n",
        "#set of masses and lambda values for binary neutron stars that agree \n",
        "\n",
        "  from scipy.interpolate import interpolate as interp\n",
        "\n",
        "  eosdir = '/content/drive/MyDrive/Colab_Notebooks/macro'\n",
        "  numeos = 2396\n",
        "\n",
        "\n",
        "  def get_mmax(eos_id):\n",
        "    eospath = eosdir+\"/macro-spec_%dcr.csv\" % eos_id\n",
        "    eos_data = np.genfromtxt(eospath, names=True, delimiter=\",\")\n",
        "    marray =  eos_data[\"M\"]\n",
        "    mmax_pos   = np.argmax(marray)\n",
        "    mmax = marray[mmax_pos]\n",
        "    return mmax\n",
        "\n",
        "  def get_lambda_from_masses(m1, m2, eos_id):\n",
        "\n",
        "    eospath = eosdir+\"/macro-spec_%dcr.csv\" % eos_id\n",
        "    eos_data = np.genfromtxt(eospath, names=True, delimiter=\",\")\n",
        "    marray, larray =  eos_data[\"M\"], eos_data[\"Lambda\"]\n",
        "    mmax_pos   = np.argmax(marray)\n",
        "    marray, larray = marray[0:mmax_pos+1], larray[0:mmax_pos+1]\n",
        "    mmax = marray[mmax_pos]\n",
        "\n",
        "    assert m1 <= mmax and m2 <= mmax, \"Error: one or both masses too heavy for a NS!\"\n",
        "\n",
        "    f_lambda = interp.interp1d(marray, larray, fill_value=0, bounds_error=False)\n",
        "    lambda1, lambda2 = f_lambda(m1), f_lambda(m2)\n",
        "\n",
        "    assert lambda2 >= lambda1, \"Error: Lambda1 is larger than Lambda2 for common EOS!\"\n",
        "\n",
        "    return lambda1, lambda2\n",
        "\n",
        "  #Dr. Landry's code requires a file that contains different equations of \n",
        "  #state(eos). Normally this data is on the LIGO supercomputers, however, since \n",
        "  #I am not using colab, I had to upload the file to drive\n",
        "\n",
        "  #-----------------------------------------------------------------------------\n",
        "  \n",
        "  import math\n",
        "  import random\n",
        "\n",
        "  #assign lambdas a float so it can initialize while loop \n",
        "  lambda1 = 5.0 \n",
        "  lambda2 = 5.0\n",
        "  \n",
        "  #lambdas below 350 does not show postmergers so it must be atleast 350 or\n",
        "  #it will only show inspiral\n",
        "  while lambda1 <= 350.0 or lambda2 <=350.0:\n",
        "\n",
        "  #picks a random eos file and gets mass  \n",
        "    eos_id = np.random.choice(range(numeos), 1)\n",
        "    mmax = get_mmax(eos_id)\n",
        "\n",
        "  #randomly picks masses between 1 and the returned mmax \n",
        "\n",
        "    m1 = random.uniform(1, mmax) \n",
        "    m2 = random.uniform(1, mmax)\n",
        "\n",
        "  # m1 has to be bigger than m2 for the proper mass ratio \n",
        "  # q calculates the mass ratio \n",
        "\n",
        "    if m1 > m2:\n",
        "      chirp_mass = (math.pow((m1 * m2), (3/5))) / (math.pow((m1 + m2), (1/5))) \n",
        "      q = m1 / m2 \n",
        "    elif m1 < m2:\n",
        "      while m1 < m2:\n",
        "        m1 = random.uniform(1, mmax)\n",
        "        m2 = random.uniform(1, mmax)\n",
        "    chirp_mass = (math.pow((m1 * m2), (3/5))) / (math.pow((m1 + m2), (1/5)))\n",
        "    q = m1 / m2\n",
        "\n",
        "  #gets the lambdas from the chosen masses\n",
        "\n",
        "    lambda1, lambda2 = get_lambda_from_masses(m1, m2, eos_id)\n",
        "\n",
        "  #dictionary parameters for making the timeseries \n",
        "\n",
        "  params = {'mchirp'       : chirp_mass,    # chirp mass [solar masses] \n",
        "              'q'          : q,      # mass ratio \n",
        "              's1x'        : 0.,      # primary spin parameter, x component\n",
        "              's1y'        : 0.,      # primary spin parameter, y component\n",
        "              's1z'        : 0.,      # primary spin parameter, z component\n",
        "              's2x'        : 0.,      # secondary spin parameter, x component\n",
        "              's2y'        : 0.,      # secondary spin parameter, y component\n",
        "              's2z'        : 0.,      # secondary spin parameter, z component\n",
        "              'lambda1'    : lambda1,    # primary tidal parameter \n",
        "              'lambda2'    : lambda2,    # secondary tidal parameter\n",
        "              'distance'   : 100.8114416513031,    # distance [Mpc]   \n",
        "              'iota'       : np.pi,   # inclination [rad]   \n",
        "              'ra'         : 0.,     # right ascension [rad]\n",
        "              'dec'        : 0.,   # declination [rad]\n",
        "              'psi'        : 0.,      # polarization angle [rad]\n",
        "              'time_shift' : 0.419,   # time shift from GPS time [s]\n",
        "              'phi_ref'    : 0.,      # phase shift [rad]\n",
        "              'f_min'      : 20.,     # minimum frequency [Hz]\n",
        "              'srate'      : srate,   # sampling rate [Hz]\n",
        "              'seglen'     : seglen,  # segment duration [s] \n",
        "              'tukey'      : 0.1,     # parameter for tukey window\n",
        "              't_gps'      : t_gps,   # GPS trigger time\n",
        "              'lmax'       : 0.,\n",
        "              'eccentricity' : 0.\n",
        "              \n",
        "             }  \n",
        "\n",
        "  #strain of time series\n",
        "\n",
        "  hp, hc = wave.compute_hphc(params)\n",
        "\n",
        "  #turn hp and hc into separate time series so we can get its amplitude and phase\n",
        "  ts_hp = TimeSeries(hp, delta_t=1/(4096*16))\n",
        "  ts_hc = TimeSeries(hc, delta_t=1/(4096*16))\n",
        "  \n",
        "  #use pycbc to get amp and phase\n",
        "  amp = waveform.utils.amplitude_from_polarizations(ts_hp, ts_hc)\n",
        "  phase = waveform.utils.phase_from_polarizations(ts_hp, ts_hc)\n",
        "\n",
        "  #additional keys for our metadata\n",
        "\n",
        "  new_params = params #put params in new dictionary\n",
        "  new_params[\"m1\"] = m1 #add m1 to new dictionary\n",
        "  new_params[\"m2\"] = m2 #add m2 to new dictionary\n",
        "  #new_params[\"mtot\"] = (m1 + m2) \n",
        "\n",
        "\n",
        " #the very first run of this cell, \"next_simulation = 1\"\n",
        " #if eos code throws an error while being iterated, change the next_simulation \n",
        " #variable to the last simulation number + 1 so if last simulation file name\n",
        " #number is \"TS00053\" then before you restart the cell, put \n",
        " #\"next_simulation = 54\" this was made when a version of this kernal kept getting \n",
        " #assert errors, but is no longer really necessary; however, it is useful for \n",
        " #having the file name start at 1 instead of 0 and that's why i kept it\n",
        "\n",
        "  next_simulation = 1\n",
        "  if num == 0:\n",
        "    new_num = num + next_simulation\n",
        "  else:\n",
        "    new_num = new_num + 1\n",
        "\n",
        "  #changes the file name for each sim run so it looks nicer imo\n",
        "\n",
        "  if new_num <= 9:\n",
        "    leading_zeros = \"TS0000\"\n",
        "  elif new_num <= 99:\n",
        "    leading_zeros = \"TS000\"\n",
        "  elif new_num <= 999:\n",
        "    leading_zeros = \"TS00\"\n",
        "  elif new_num <= 9999:\n",
        "    leading_zeros = \"TS0\"\n",
        "  else:\n",
        "    leading_zeros = \"TS\"\n",
        "\n",
        "\n",
        "  #assigning arguments into variables so it can be changed easily\n",
        "  #new_params[\"file name\"] changes the dictionary key to have the associated \n",
        "  #time series csv filename within the metadata\n",
        "  new_params[\"file name\"] = f\"{leading_zeros}{new_num}.csv\"\n",
        "\n",
        "  #text_file is the path for the time series files to save in\n",
        "  text_file = f'/content/drive/MyDrive/Colab_Notebooks/practice/{leading_zeros}{new_num}.csv'\n",
        "\n",
        "\n",
        "\n",
        "  #separates the time series into two columns so it can be in a dataframe\n",
        "\n",
        "  x = series.times\n",
        "  ts_df= pd.DataFrame({'time': x, 'amplitude': amp, 'h_plus': hp, 'h_cross': hc, \n",
        "                       'phase': phase})\n",
        "\n",
        "\n",
        "\n",
        "  #deletes rows from original dataframe so the time series csv will only be \n",
        "  #between -0.6 and +0.6 seconds with srate = 4096\n",
        "\n",
        "  #ts_df.drop(ts_df.index[0:16138], 0, inplace = True)\n",
        "  #ts_df.drop(ts_df.index[493:], 0, inplace = True)\n",
        "\n",
        "  #deletes rows from original dataframe so the time series csv will only be \n",
        "  #between -0.6 and +0.6 seconds with srate = 4096 * 16; however, since the time\n",
        "  #series needs to be within a power of 2, we must adjust the dataframe to be \n",
        "  #within 8192 rows dropping the first 258049 and keeps only 8192 rows to land \n",
        "  #the time series between -0.6... and +0.6... seconds\n",
        "  ts_df.drop(ts_df.index[0:258049], 0, inplace = True)\n",
        "  ts_df.drop(ts_df.index[8192:], 0, inplace = True)\n",
        "\n",
        "\n",
        "  #formats dataframe as a csv file \n",
        "  \n",
        "  ts_df.to_csv(text_file, sep= '\\t', header = None, index = False) \n",
        "\n",
        "\n",
        "  #creates original metadata file then appends the subsequent iterations\n",
        "  if new_num == 1:\n",
        "    df = pd.DataFrame.from_dict([new_params])\n",
        "    df.to_csv('/content/drive/MyDrive/Colab_Notebooks/practice/PRacticeMETADATA.csv', \n",
        "              sep= '\\t', index = False)\n",
        "  else: \n",
        "    from csv import DictWriter\n",
        "\n",
        "    def append_dict_as_row(file_name, dict_of_elem, field_names):\n",
        "      # Open file in append mode\n",
        "        with open(file_name, 'a+', newline='') as write_obj:\n",
        "          # Create a writer object from csv module\n",
        "            dict_writer = DictWriter(write_obj, delimiter = '\\t', fieldnames=field_names)\n",
        "            # Add dictionary as row in the csv\n",
        "            dict_writer.writerow(dict_of_elem)\n",
        "\n",
        "    field_names = ['mchirp', 'q', 's1x', 's1y', 's1z', 's2x', 's2y', 's2z', \n",
        "               'lambda1', 'lambda2', 'distance','iota', 'ra', 'dec', 'psi',\n",
        "               'time_shift','phi_ref', 'f_min', 'srate', 'seglen', 'tukey', \n",
        "               't_gps', 'lmax', 'eccentricity', 'mtot','m1', 'm2', 'file name'\n",
        "               ]  \n",
        "    # Adds each simulation as a row in the metadata csv file\n",
        "    row_dict = new_params\n",
        "    append_dict_as_row('/content/drive/MyDrive/Colab_Notebooks/practice/PRacticeMETADATA.csv', \n",
        "                       row_dict, field_names)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}