{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timeseries_mastercopy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x0tyVUhCVFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197ecf19-289b-423d-c596-6741f39ba54a"
      },
      "source": [
        "#Originally used \"!pip install pycbc lalsuite ligo-common\" and \"!pip install gwpy\" but something about installing both pycbc and gwpy \n",
        "#causes colab to give errors in compatibilities, as well as, \"missing\" packages that asks you to restart runtime; dont restart runtime, just restart cell of pycbc\n",
        "#installing an older version of gwpy or else a compatibility error happens\n",
        "\n",
        "! pip install -q 'lalsuite==6.66' 'gwpy==1.0.1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 28.5 MB 68 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 22.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 559 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 16.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 27.6 MB/s \n",
            "\u001b[?25h  Building wheel for ligo-segments (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lscsoft-glue (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0h8IFwLfBpI"
      },
      "source": [
        "#running cell that imports gwpy near the top so if there's any errors, there won't be time wasted restarting the runtime and running cells again\n",
        "\n",
        "import gwpy\n",
        "from gwpy.timeseries import TimeSeries"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRXMoY7HBLRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac54a26-8303-40df-e500-de8e676918bf"
      },
      "source": [
        "#clone github repo of bajes\n",
        "\n",
        "! git clone https://github.com/matteobreschi/bajes.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bajes'...\n",
            "remote: Enumerating objects: 223, done.\u001b[K\n",
            "remote: Counting objects: 100% (223/223), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 223 (delta 30), reused 223 (delta 30), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (223/223), 11.93 MiB | 16.94 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXePtjaxBmW3"
      },
      "source": [
        "#installs bajes repo to your content files on google colab, if using linux itll have its own directory\n",
        "\n",
        "%cd bajes\n",
        "! ls\n",
        "! python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFRkA4cCo6h1"
      },
      "source": [
        "#necessary to run \"TEOBResumS_NRPM\" approx but not necessary if using \"NRPM\" approx\n",
        "#however, if using \"NRPM\", lambda tides must >= 400 in it's paramaters in the dictionary cell a few cells down\n",
        "\n",
        "! git clone https://RoxGamba@bitbucket.org/eob_ihes/teobresums.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsC6g8Wko9aF"
      },
      "source": [
        "#TEOBResumS wont install properly unless installing the build-essentials and lib*\n",
        "\n",
        "! apt update\n",
        "! apt install build-essential\n",
        "! apt-get install -y libconfig-dev\n",
        "!apt-get install libgsl-dev\n",
        "% cd teobresums/Python/\n",
        "!python TEOBResumSWrap_setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIQ79R4HCljv"
      },
      "source": [
        "#Importing some packages\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#depending on how bajes uploads on google colab or locally, the path for bajes might be different. \n",
        "#instead of \"bajes.bajes.obs...\" it might just be \"bajes.obs...\"\n",
        "\n",
        "from bajes.bajes.obs.gw import Series\n",
        "from bajes.bajes.obs.gw import Noise\n",
        "from bajes.bajes.obs.gw.utils import read_asd\n",
        "import csv"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exgz3VaL-5eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8426037a-ad71-4d06-afef-4e54f6651b1d"
      },
      "source": [
        "#import google drive so files we are making wont disappear after runtime is over. \n",
        "#this is unnecessary if not using google colab\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/', force_remount = True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJOgMXMV1E"
      },
      "source": [
        "#As of this moment there are 11 known events detected from the advanced gravitational-wave detectors\n",
        "#we can access the data from the known events from the gwpy package and the time of event currently listed is from GW150914\n",
        "#Waveform approximants(bottom of this cell) can be run independently from the event data, however, hard coding/replacing \"'time'\", \"hpc.plus\", etc and \n",
        "#other adjustments of the code made it difficult for me to change the central time to equal zero so accessing the data was easier\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "#should the need to not use the data from events occur, \n",
        "#comment out everything above \"#set the data properties\", comment out \"series\", and add: \n",
        "\n",
        "#dT     = 1/srate\n",
        "#df = 1/seglen\n",
        "#f = np.arange(0, srate/2, df)\n",
        "#wave  = Waveform(f, srate, seglen, 'NRPM')\n",
        "\n",
        "#then in the next cell, comment out \"hpc\" and add:\n",
        "\n",
        "#hp, hc = wave.compute_hphc(params)\n",
        "# t = np.array(range(0, len(hp)))*dT\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "time_of_event = 1126259462.4\n",
        "\n",
        "post_trigger_duration =4\n",
        "duration = 8\n",
        "analysis_start = time_of_event + post_trigger_duration - duration\n",
        "\n",
        "# Use gwpy to fetch the open data\n",
        "H1_analysis_data = TimeSeries.fetch_open_data(\n",
        "    \"H1\", analysis_start, analysis_start + duration, sample_rate=4096, cache=True)\n",
        "\n",
        "t = H1_analysis_data.times\n",
        "strain = H1_analysis_data.value\n",
        "\n",
        "# set the data properties \n",
        "seglen = 8           # duration of the segment [s]\n",
        "srate  = 4096  * 16       # sampling rate [Hz]\n",
        "t_gps  = 0  # central value of GPS time\n",
        "f_max  = 1024 * 4\n",
        "f_min  = 20 \n",
        "\n",
        "series = Series('time', strain, seglen=seglen, srate=srate, t_gps=t_gps, f_min=f_min, f_max=f_max)\n",
        "print('here')\n",
        "\n",
        "from bajes.bajes.obs.gw import Waveform\n",
        "\n",
        "wave  = Waveform(series.freqs, srate, seglen, 'TEOBResumS_NRPM') #NRPM for postmergers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD1xO7KJVvC_"
      },
      "source": [
        "!pip install pycbc\n",
        "import pylab\n",
        "\n",
        "from pycbc import waveform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNdAt8-GYxAV"
      },
      "source": [
        "from pycbc.waveform import get_td_waveform\n",
        "from pycbc.types import TimeSeries, FrequencySeries, Array, float32, float64, complex_same_precision_as, real_same_precision_as"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNTl2OSBSBwK"
      },
      "source": [
        "#since we want a lot of waveforms, we run our this cell with a for loop\n",
        "#change the range number based on how many sims you want\n",
        "\n",
        "for num in range(1):\n",
        "\n",
        "  #----------------------------------------------------------------------------------------\n",
        "\n",
        "  #This part of code is from Dr. Phillip Landry and what it does is deternmine a set of masses and \n",
        "  #lambda values for binary neutron stars that agree with each other\n",
        "\n",
        "  from scipy.interpolate import interpolate as interp\n",
        "\n",
        "  eosdir = '/content/drive/MyDrive/Colab_Notebooks/macro'\n",
        "  numeos = 2396\n",
        "\n",
        "\n",
        "  def get_mmax(eos_id):\n",
        "    eospath = eosdir+\"/macro-spec_%dcr.csv\" % eos_id\n",
        "    eos_data = np.genfromtxt(eospath, names=True, delimiter=\",\")\n",
        "    marray =  eos_data[\"M\"]\n",
        "    mmax_pos   = np.argmax(marray)\n",
        "    mmax = marray[mmax_pos]\n",
        "    return mmax\n",
        "\n",
        "  def get_lambda_from_masses(m1, m2, eos_id):\n",
        "\n",
        "    eospath = eosdir+\"/macro-spec_%dcr.csv\" % eos_id\n",
        "    eos_data = np.genfromtxt(eospath, names=True, delimiter=\",\")\n",
        "    marray, larray =  eos_data[\"M\"], eos_data[\"Lambda\"]\n",
        "    mmax_pos   = np.argmax(marray)\n",
        "    marray, larray = marray[0:mmax_pos+1], larray[0:mmax_pos+1]\n",
        "    mmax = marray[mmax_pos]\n",
        "\n",
        "    assert m1 <= mmax and m2 <= mmax, \"Error: one or both masses too heavy for a NS!\"\n",
        "\n",
        "    f_lambda = interp.interp1d(marray, larray, fill_value=0, bounds_error=False)\n",
        "    lambda1, lambda2 = f_lambda(m1), f_lambda(m2)\n",
        "\n",
        "    assert lambda2 >= lambda1, \"Error: Lambda1 is larger than Lambda2 for common EOS!\"\n",
        "\n",
        "    return lambda1, lambda2\n",
        "\n",
        "  #Dr. Landry's code requires a file that contains different equations of state(eos). Normally this data\n",
        "  #is on the LIGO supercomputers, however, since I am not using colab, I had to upload the file to drive\n",
        "\n",
        "  #-------------------------------------------------------------------------------------------------------------\n",
        "  \n",
        "  import math\n",
        "  import random\n",
        "\n",
        "  #picks a random eos file and gets mass  \n",
        "\n",
        "  eos_id = np.random.choice(range(numeos), 1)\n",
        "  mmax = get_mmax(eos_id)\n",
        "\n",
        "  #randomly picks masses between 1 and the returned mmax \n",
        "\n",
        "  m1 = random.uniform(1, mmax) \n",
        "  m2 = random.uniform(1, mmax)\n",
        "\n",
        "  # m1 has to be bigger than m2 for the proper mass ratio \n",
        "  # q calculates the mass ratio \n",
        "\n",
        "  if m1 > m2:\n",
        "    chirp_mass = (math.pow((m1 * m2), (3/5))) / (math.pow((m1 + m2), (1/5))) \n",
        "    q = m1 / m2 \n",
        "  elif m1 < m2:\n",
        "    while m1 < m2:\n",
        "      m1 = random.uniform(1, mmax)\n",
        "      m2 = random.uniform(1, mmax)\n",
        "  chirp_mass = (math.pow((m1 * m2), (3/5))) / (math.pow((m1 + m2), (1/5)))\n",
        "  q = m1 / m2\n",
        "\n",
        "  #gets the lambdas from the chosen masses\n",
        "\n",
        "  lambda1, lambda2 = get_lambda_from_masses(m1, m2, eos_id)\n",
        "\n",
        "  #dictionary parameters for making the timeseries \n",
        "\n",
        "  params = {'mchirp'       : chirp_mass,    # chirp mass [solar masses] \n",
        "              'q'          : q,      # mass ratio \n",
        "              's1x'        : 0.,      # primary spin parameter, x component\n",
        "              's1y'        : 0.,      # primary spin parameter, y component\n",
        "              's1z'        : 0.,      # primary spin parameter, z component\n",
        "              's2x'        : 0.,      # secondary spin parameter, x component\n",
        "              's2y'        : 0.,      # secondary spin parameter, y component\n",
        "              's2z'        : 0.,      # secondary spin parameter, z component\n",
        "              'lambda1'    : lambda1,    # primary tidal parameter \n",
        "              'lambda2'    : lambda2,    # secondary tidal parameter\n",
        "              'distance'   : 100.8114416513031,    # distance [Mpc]   \n",
        "              'iota'       : np.pi,   # inclination [rad]   \n",
        "              'ra'         : 0.,     # right ascension [rad]\n",
        "              'dec'        : 0.,   # declination [rad]\n",
        "              'psi'        : 0.,      # polarization angle [rad]\n",
        "              'time_shift' : 0.419,   # time shift from GPS time [s]\n",
        "              'phi_ref'    : 0.,      # phase shift [rad]\n",
        "              'f_min'      : 20.,     # minimum frequency [Hz]\n",
        "              'srate'      : srate,   # sampling rate [Hz]\n",
        "              'seglen'     : seglen,  # segment duration [s] \n",
        "              'tukey'      : 0.1,     # parameter for tukey window\n",
        "              't_gps'      : t_gps,   # GPS trigger time\n",
        "              'lmax'       : 0.,\n",
        "              'eccentricity' : 0.\n",
        "             }  \n",
        "\n",
        "  #strain of time series\n",
        "\n",
        "  hp, hc = wave.compute_hphc(params)\n",
        "\n",
        "  #turn hp and hc into a separate time series so we can get its amplitude and phase\n",
        "  ts_hp = TimeSeries(hp, delta_t=1/(4096*16))\n",
        "  ts_hc = TimeSeries(hc, delta_t=1/(4096*16))\n",
        "  \n",
        "  #use pycbc to get amp and phase\n",
        "  amp = waveform.utils.amplitude_from_polarizations(ts_hp, ts_hc)\n",
        "  phase = waveform.utils.phase_from_polarizations(ts_hp, ts_hc)\n",
        "\n",
        "  #additional keys for our metadata\n",
        "\n",
        "  new_params = params #put params in new dictionary\n",
        "  new_params[\"m1\"] = m1 #add m1 to new dictionary\n",
        "  new_params[\"m2\"] = m2 #add m2 to new dictionary\n",
        "  new_params[\"mtot\"] = round((m1 + m2), 1) #change mtot from params into a float with only 1 decimal point\n",
        "\n",
        "\n",
        " #the very first run of this cell, \"next_simulation = 1\"\n",
        " #if eos code throws an error while being iterated, change the next_simulation variable to the last simulation number + 1 so if \n",
        " #last simulation file name number is \"S00053\" then before you restart the cell, put \"next_simulation = 54\"\n",
        " #this was made when a version of this kernal kept getting assert errors, but is no longer really necessary; however,\n",
        " #it is useful for having the file name start at 1 instead of 0 and that's why i kept it\n",
        "\n",
        "  next_simulation = 1\n",
        "  if num == 0:\n",
        "    new_num = num + next_simulation\n",
        "  else:\n",
        "    new_num = new_num + 1\n",
        "\n",
        "  #changes the file name for each sim run so it looks nicer imo\n",
        "\n",
        "  if new_num <= 9:\n",
        "    leading_zeros = \"S0000\"\n",
        "  elif new_num <= 99:\n",
        "    leading_zeros = \"S000\"\n",
        "  elif new_num <= 999:\n",
        "    leading_zeros = \"S00\"\n",
        "  elif new_num <= 9999:\n",
        "    leading_zeros = \"S0\"\n",
        "  else:\n",
        "    leading_zeros = \"S\"\n",
        "\n",
        "\n",
        "  #assigning arguments into variables so it can be changed easily\n",
        "  #new_params[\"file name\"] changes the dictionary key to have the associated time series csv filename within the metadata\n",
        "  new_params[\"file name\"] = f\"{leading_zeros}{new_num}.csv\"\n",
        "\n",
        "  #text_file is the path for the time series files to save in\n",
        "  text_file = f'/content/drive/MyDrive/Colab_Notebooks/{leading_zeros}{new_num}.csv'\n",
        "\n",
        "\n",
        "\n",
        "  #separates the time series into two columns so it can be in a dataframe\n",
        "\n",
        "  x = series.times\n",
        "  ts_df= pd.DataFrame({'time': x, 'amplitude': amp, 'h_plus': hp, 'h_cross': hc, 'phase': phase})\n",
        "\n",
        "\n",
        "\n",
        "  #deletes rows from original dataframe so the time series csv will only be between -0.6 and +0.6 seconds with srate = 4096\n",
        "  #ts_df.drop(ts_df.index[0:16138], 0, inplace = True)\n",
        "  #ts_df.drop(ts_df.index[493:], 0, inplace = True)\n",
        "\n",
        "  #deletes rows from original dataframe so the time series csv will only be between -0.6 and +0.6 seconds with srate = 4096 * 16\n",
        "  #however, since the time series needs to be within a power of 2, we must adjust the dataframe to be within 8192 rows\n",
        "  #dropping the first 258049 and keeps only 8192 rows to land the time series between -0.6... and +0.6... seconds\n",
        "  ts_df.drop(ts_df.index[0:258049], 0, inplace = True)\n",
        "  ts_df.drop(ts_df.index[8192:], 0, inplace = True)\n",
        "\n",
        "\n",
        "  #formats dataframe\n",
        "  \n",
        "  ts_df.to_csv(text_file, sep= '\\t', header = None, index = False) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #creates original metadata file then appends the subsequent iterations\n",
        "  if new_num == 1:\n",
        "    df = pd.DataFrame.from_dict([new_params])\n",
        "    df.to_csv('/content/drive/MyDrive/Colab_Notebooks/METADATA_FILENAME_PRACTICE.csv', sep= '\\t', index = False)\n",
        "  else: \n",
        "    from csv import DictWriter\n",
        "\n",
        "    def append_dict_as_row(file_name, dict_of_elem, field_names):\n",
        "      # Open file in append mode\n",
        "        with open(file_name, 'a+', newline='') as write_obj:\n",
        "          # Create a writer object from csv module\n",
        "            dict_writer = DictWriter(write_obj, delimiter = '\\t', fieldnames=field_names)\n",
        "            # Add dictionary as row in the csv\n",
        "            dict_writer.writerow(dict_of_elem)\n",
        "\n",
        "    field_names = ['mchirp', 'q', 's1x', 's1y', 's1z', 's2x', 's2y', 's2z', \n",
        "               'lambda1', 'lambda2', 'distance','iota', 'ra', 'dec', 'psi','time_shift',\n",
        "               'phi_ref', 'f_min', 'srate', 'seglen', 'tukey', 't_gps', 'lmax', 'eccentricity', 'm1', 'm2', 'mtot', 'file name'\n",
        "               ]  \n",
        "    # Adds each simulation as a row in the metadata csv file\n",
        "    row_dict = new_params\n",
        "    append_dict_as_row('/content/drive/MyDrive/Colab_Notebooks/METADATA_FILENAME_PRACTICE.csv', row_dict, field_names)\n",
        "\n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": []
    }
  ]
}